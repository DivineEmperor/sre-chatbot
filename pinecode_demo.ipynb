{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(directory):\n",
    "    loader = PyPDFDirectoryLoader(directory)\n",
    "    files = loader.load()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_doc(\"./documents\")\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=800,chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_documents(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = chunk_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = embeddings.embed_query(\"Hello, world!\")\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "#replace the api_key with your own\n",
    "\n",
    "pc = Pinecone(api_key=\"\")\n",
    "index = pc.Index(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate OpenAI embeddings\n",
    "def get_openai_embeddings(text):\n",
    "    response = openai.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    # Access the embedding directly from the 'data' field\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "# Prepare your data for insertion into Pinecone\n",
    "def prepare_data_for_pinecone(documents):\n",
    "    data = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Generate embeddings for the document content\n",
    "        embedding = get_openai_embeddings(doc.page_content)  # Use the actual page content for embeddings\n",
    "        \n",
    "        # Each entry will have a unique ID, the embedding, and metadata including the actual content\n",
    "        metadata = {\n",
    "            'source': doc.metadata['source'],  # Add the source (like file name)\n",
    "            'page': doc.metadata['page'],      # Add the page number\n",
    "            'content': doc.page_content        # Add the actual text content\n",
    "        }\n",
    "        \n",
    "        # Append the document with ID, embedding, and metadata (which now includes text content)\n",
    "        data.append((f\"doc_{i}\", embedding, metadata))  # f\"doc_{i}\" is a unique ID\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the embeddings into Pinecone\n",
    "def insert_into_pinecone(data):\n",
    "    index.upsert(vectors=data)\n",
    "\n",
    "# Prepare and insert all documents into Pinecone\n",
    "data_for_pinecone = prepare_data_for_pinecone(docs)\n",
    "insert_into_pinecone(data_for_pinecone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the documents are stored by querying the index\n",
    "def query_pinecone(query_text):\n",
    "    try:\n",
    "        # Generate embedding for the query text\n",
    "        query_embedding = get_openai_embeddings(query_text)\n",
    "\n",
    "        # Debugging: Check the length of the embedding\n",
    "        print(f\"Query embedding length: {len(query_embedding)}\")\n",
    "        \n",
    "        # Ensure embedding is the correct length for the Pinecone index\n",
    "        if len(query_embedding) != 1536:\n",
    "            raise ValueError(f\"Embedding dimension mismatch: Expected 1536, got {len(query_embedding)}\")\n",
    "\n",
    "        # Perform the query in Pinecone using the updated syntax\n",
    "        query_result = index.query( # You can set a custom namespace if necessary\n",
    "            vector=query_embedding,  # Use vector instead of queries\n",
    "            top_k=5,  # Return top 5 matches\n",
    "            include_values=False,  # Whether to include the vectors in the result\n",
    "            include_metadata=True  # Example filter (modify as per your use case)\n",
    "        )\n",
    "\n",
    "        # Check if matches are found\n",
    "        if query_result['matches']:\n",
    "            # Print the relevant matches with metadata and score\n",
    "            return query_result\n",
    "        else:\n",
    "            print(\"No matches found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any potential errors in querying\n",
    "        print(f\"Error querying Pinecone: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pinecone(\"Budget priorities for 2023-24\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_answer_from_openai(context, question):\n",
    "    try:\n",
    "        # Initialize the OpenAI client\n",
    "        client = OpenAI()\n",
    "        \n",
    "        # Combine context and question into the messages array\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"The following is relevant context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "        \n",
    "        # Use the new way to call chat completions\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Use the appropriate model as per your requirement\n",
    "            messages=messages,\n",
    "            max_tokens=150,  # Adjust based on how long you want the answer to be\n",
    "            temperature=0.2  # Adjust for more or less randomness\n",
    "        )\n",
    "        \n",
    "        # Extract the content of the response\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer from OpenAI: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now combine the two functions (query and answer generation)\n",
    "def answer_question_using_context(question):\n",
    "    try:\n",
    "        # Step 1: Query Pinecone for relevant context\n",
    "        query_result = query_pinecone(question)\n",
    "        \n",
    "        # Step 2: If context is found, combine it and pass to OpenAI\n",
    "        if query_result:\n",
    "            context = \"\"\n",
    "            for match in query_result['matches']:\n",
    "                context += f\"ID: {match['id']}, Metadata: {match['metadata']}, Content: {match['metadata']['content']}\\n\"\n",
    "            \n",
    "            # Step 3: Get the answer from OpenAI based on the context\n",
    "            answer = get_answer_from_openai(context, question)\n",
    "            \n",
    "            return answer\n",
    "        else:\n",
    "            return \"No relevant context found in Pinecone.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error answering the question: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "question = \"What are the main priorities of the 2023-24 Indian Budget?\"\n",
    "answer = answer_question_using_context(question)\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
